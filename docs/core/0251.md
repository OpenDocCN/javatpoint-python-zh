# Python 数据分析

> 原文:[https://www.javatpoint.com/python-data-analytics](https://www.javatpoint.com/python-data-analytics)

数据分析可以帮助我们从数据中获取有用的信息，并为我们的查询提供解决方案。此外，基于观察到的模式，我们可以预测不同商业政策的结果。

## 了解数据分析的基础

### 数据

我们在分析过程中处理的数据类型主要是 csv(逗号分隔值)格式。通常，csv 文件中的第一行表示为标题。

### 可用包

[Python](https://www.javatpoint.com/python-tutorial) 包中有各种各样的库，可以在不编写长代码的情况下轻松实现。

一些包的例子是-

1.  科学计算库，如 NumPy、Pandas & SciPy。
2.  可视化库，如 Matplotlib 和 seaborn。
3.  算法库，如 scikit-learn 和 statsmodels。

### 导入和导出数据集

导入数据集时，我们必须注意的两个基本事项是-

1.  **格式-** 指的是文件的编码方式。突出格式的例子有。csv，。xlsx，。json 等。
2.  **文件路径-** 文件的路径是指文件存放的位置。它可以在任何驱动器或一些在线源中提供。

可以通过以下方式完成-

**示例-**

```py

import pandas as pd
path=" "
df = pd.read_csv(path)

```

如果数据集不包含标题，我们可以通过以下方式指定它-

```py

df = pd.read_csv(path,header=None)

```

为了查看数据集的前五行和后五行，我们可以分别使用 df.head()和 df.tail()。

让我们看看如何导出数据，如果我们在。csv 格式，然后

```py

path = " "
df.to_excel(path)

```

## 数据争论

数据争论是将数据从原始格式转换为可用于分析的格式的过程

让我们看看这部分包含了什么-

### 如何处理缺失值？

缺少值-由于信息不可用，一些条目留空。它通常用 NaN、？或者 0。

让我们讨论一下如何处理它们-

最好的选择是用平均值替换数值变量，用模式替换分类变量。

有时可能会出现这样的情况，当我们不得不丢弃丢失的值时，可以使用-

```py

df.dropna() 

```

如果我们想删除一行，我们必须将轴指定为 0。如果我们想删除一列，我们必须指定轴为 1。

此外，如果我们希望这些变化直接发生在数据集中，我们将在 place = True 中再指定一个参数**。**

现在让我们看看如何替换这些值-

语法是-

```py

df.replace(missing value, new value)

```

在这里，我们将创建一个变量，并在其中存储属性的平均值(我们想要替换它的值)

```py

mean=df["attribute name"].mean()
df["attribute name"].replace(np.nan,mean)

```

### 如何进行数据格式化？

它指的是以可理解的格式提供数据的过程。例如-更改变量名使其易于理解。

### 数据标准化

数据集中存在的要素的值可能会导致有偏差的预测。因此，我们必须把它们带到一个可以比较的范围。

为了做到这一点，我们可以在属性上使用以下技术-

1.  简单功能扩展 Xn=Xold/Xmax
2.  最小-最大方法 Xn=Xold-Xmin/Xmax-Xmin
3.  z-score xn = xold-/ꝺ
    -平均值
    ꝺ-standard 偏差

### 如何将分类变量转换成数值变量？

在这种情况下，我们继续进行一个称为“一热编码”的过程，假设有一个属性保存分类值。我们将根据可能性制造虚拟变量，并根据它们在属性中的出现情况为它们分配 0 或 1。

要将分类变量转换为虚拟变量 0 或 1，我们将使用

```py

pandas.get_dummies(df["attribute-name"])
This will generate the expected results.

```

### python 中的绑定

它指的是将数值变量转换为分类变量的过程。

假设我们从数据集中获取了属性“价格”。我们可以根据范围将它的数据分为三类，然后用低价、中价、高价等名称来表示。

我们可以使用 **linspace()** 方法获得范围

```py

bin = np.linspace(min(df["attribute-name"]),max(df["attribute-name"]),4)
cat_names=["low-price","mid-price","high-price"]
df["bin_name"]=pd.cut(df["attribute-name"],bin,labels=cat_names)

```

## 探索性数据分析

### 统计数字

我们可以使用**description()**方法找出我们数据集的统计概要。可用作**df . description()。**分类变量可以使用值 **_counts()** 方法进行总结。

### 使用 GroupBy

熊猫的 groupby()方法可以应用于分类变量。它根据不同的类别对子集进行分组。它可以包含单个或多个变量。

让我们看一个例子，它将帮助我们理解如何在 Python 中使用它。

```py

df_att=df[['attribute1', 'attribute2', 'attribute3']]
df_g=df_att.groupby(['attribute1', 'attribute2'], as_index=False).mean()
df_g

```

### 相互关系

相关性衡量两个变量相互依赖的范围。

检查两个变量之间存在何种相关性的直观想法。我们可以绘制一个图表，并解释一个属性的值的增加如何影响另一个属性。

关于统计量，我们可以使用皮尔逊相关来获得相关性。它给出了相关系数和 P 值。

让我们看看标准-

| 相关系数 | 关系 |
| 1.接近+1 | 大正 |
| 2.接近-1 | 大底片 |
| 3.接近 0 | 不存在任何关系 |

| p 值 | 确定性 |
| p 值< 0.001 | 强烈的 |
| p 值< 0.05 | 温和的 |
| p 值< 0.1 | 无力的 |
| p 值> 0.1 | 不 |

我们可以在使用 scipy stat 包的代码中使用它。

假设我们想要计算两个属性之间的相关性，属性 1 和属性 2-

```py

pearson_coef,p_value=stats.pearsonr(df["attribute1"],df["attribute2"]).

```

为了进一步检查所有变量之间的相关性，我们可以创建一个热图。

### 两个分类变量之间的关系

两个分类变量之间的关系可以用卡方方法计算。

```py

scipy.stats.chi2_contingency(cont_table, correction=True)

```

## 如何开发模型？

首先，让我们了解什么是模型？

模型可以指帮助我们预测结果的方程。

*   线性回归和多元线性回归

**线性回归-** 顾名思义，只涉及单个自变量进行预测。

**多元回归-** 涉及多个自变量进行预测。

简单的线性回归方程可以表示为-

y=b <sub>0</sub> x+b <sub>1</sub>

在这里，

y 因变量

x 独立变量

b<sub>0</sub>-坡度

b<sub>1</sub>-拦截

用 Python 实现线性回归

```py

from sklearn.linear_model import LinearRegression
lm=LinearRegression()
X=df["attribute-name1"]
Y=df["attribute-name1"]
lm.fit(X,Y)
yp=lm.predict(X)

```

### 使用可视化评估我们的模型

创建情节是一个很好的实践，因为它们显示了相关性的强度，以及关系的方向是积极的还是消极的。

让我们看看不同的图，这些图可以帮助我们评估我们的模型-

1.使用回归图

```py

import seaborn as sns
sns.regplot(x="attribute1",y="attribute2",data=df)
plt.ylim(0,)

```

2.使用残差图

```py

import seaborn as sns
sns.residplot(df["attribute1"],df["attribute2"])

```

### 在样本评估中

在这里，我们将讨论如何对我们的模型进行数值评估，两种方法是-

**1。均方误差**

这种方法将实际值和预测值之间的差值进行平方，然后最终计算出它们的平均值。

我们可以用 Python 实现同样的功能

```py

from sklearn.metrics 
import mean_squared_error
mean_squared_error(df['target-variable'],Y_predict_simple_fit)

```

**2。r 平方**

r 平方也称为决定系数。它显示了数据与拟合回归线的接近程度。可以使用 **score()** 方法在 Python 中使用。

```py

X=df["attribute-1"]
Y=df["attribute-2"]
lm.fit(X,Y)
lm.score(X,Y)

```

### 决策

简而言之，当我们评估一个模型时，我们必须注意以下几点

1.  使用可视化
2.  使用数值评估方法。

## 如何评价一个模型？

评估我们的模型是一个不可或缺的因素，因为它告诉我们的数据与模型的吻合程度。现在，我们将讨论如何使用训练数据来预测结果。

关键的想法是将我们的数据集分为训练和测试。训练数据集用于构建我们的模型，测试数据集用于评估我们模型的性能。

它可以用 Python 实现，使用-

```py

 from sklearn.model_selection import train_test_split
x_train,y_train,x_test,y_test=train_test_split(x_data,y_data,test_size=' ',random_state=' ')

```

### 过拟合和欠拟合

**过拟合-** 是模型拟合数据相当简单的情况。

**欠拟合-** 是模型容易调整噪声因子而不是函数的情况。

### 里脊回归

这是在我们处理十次变量时使用的。这里我们引入了一个叫做α的因子。让我们看看如何在 Python 中实现这一点。

```py

from sklearn.linear_model import Ridge
RModel=Ridge(alpha=0.1)
RModel.fit(X,Y)
Yp=RModel.predict(X)

```

* * *