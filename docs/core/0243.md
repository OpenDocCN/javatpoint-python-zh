# 梯度下降算法

> 原文:[https://www.javatpoint.com/gradient-descent-algorithm](https://www.javatpoint.com/gradient-descent-algorithm)

梯度下降算法是许多机器学习算法中用来最小化代价函数的优化算法。梯度下降算法用于更新学习模型的参数。

以下是不同类型的梯度下降:

*   **批量梯度下降:**批量梯度下降是一种梯度算法，用于处理梯度下降每次迭代的所有训练数据集。假设训练数据集的数量很大，批量梯度下降会比较昂贵。因此，如果训练数据集的数量很大，则不建议用户使用批量梯度下降。相反，他们可以对大型训练数据集使用小批量梯度下降。
*   **小批量梯度下降:**小批量梯度下降是用于比其他两种梯度下降工作更快的梯度下降类型。假设用户有“ **p** ”(其中“ **p** ”是批量梯度下降)数据集，其中每次迭代将处理 **p < m** (其中“ **m** ”是小批量梯度下降)。因此，即使“ **p** ”训练数据集的数量很大，小批量梯度下降也会在一次尝试中分批处理“ **p** ”训练数据集。因此，它可以用较少的迭代次数处理大型训练数据集。
*   **随机梯度下降:**随机梯度下降是梯度下降的类型，每次迭代可以处理一个训练数据集。因此，参数将在每次迭代后更新，在每次迭代中只处理了一个数据集。这种梯度下降比批量梯度下降更快。但是，如果训练数据集的数量很大，那么它一次将只处理一个数据集。因此，迭代次数会很大。

### 使用的变量:

设**‘k’**为训练数据集的个数。

让**‘j’**为数据集中的要素数量。

如果 **p == k** ，小批量梯度下降的行为将类似于批量梯度下降。(其中**‘p’**为批量梯度下降)

## 用于批量梯度下降的算法:

设 **h <sub>θ</sub> (a)** 为线性回归的假设。那么成本函数将由下式给出:

让**σ**代表从 **t = 1** 到 **k** 的所有训练数据集的总和。

```
Gtrain(θ) = (1/2k) Σ (hθ(a(t)) - b(t))2

Repeat {
θg = θg - (learning rate/k) * Σ (hθ(a(t)) - b(t))ag(t)
   For every g = 0 …j
}

```

其中 **a <sub>g</sub> <sup>(t)</sup>** 代表 **t <sup>th</sup>** 训练数据集的 **g <sup>th</sup>** 特征，假设如果“ **k** 非常大(例如，700 万个训练数据集)，那么批量梯度下降将花费数小时甚至数天来完成该过程。因此，对于大的训练数据集，不建议用户使用批量梯度下降，因为这会减慢机器的学习过程。

## 用于小批量梯度下降的算法

假设“ **p** ”是一批中数据集的数量，其中 **p < k.**

让**p = 10****k = 100**；

然而，用户可以调整批量大小。这通常写成 2 的幂。

```
重复{对于 t = 1，11，21，…..，91 设σ为 d .θ<sub>g</sub>=θ<sub>g</sub>-(学习率/大小(p))*σ(h<sub>θ</sub>(a<sup>(d)</sup>)-b<sup>(d)</sup>)a<sub>g</sub><sup>(d)</sup>每 g = 0 …j  } 
```

### 用于随机梯度下降的算法；

*   这种梯度下降将随机打乱数据集，为每种数据训练参数。
*   随机梯度下降每次迭代只需要一个数据集。

```
Hence,
Let (a(t), b(t)) be the training dataset
Cost(θ, (a(t), b(t))) = (1/2) Σ (hθ(a(t)) - b(t))2

Gtrain(θ) = (1/k) Σ Cost (θ, (a(t), b(t)))

Repeat {
  For t = 1 to k{
      Θg = θg - (learning rate) * Σ (hθ(a(t)) - b(t))ag(t)
	    For every g = 0 …j

		}
}

```

## 结论

在本教程中，我们已经讨论了不同类型的梯度下降算法及其变体。

* * *